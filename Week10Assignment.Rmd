---
title: "week10"
author: "Andreina A"
date: "2024-11-10"
output: html_document
---

```{r}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(ggplot2)
library(gutenbergr)
library(wordcloud)
```

## Introduction

In Text Mining with R, Chapter 2 looks at Sentiment Analysis.  In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document.  You should provide a citation to this base code.  You’re then asked to extend the code in two ways:

Work with a different corpus of your choosing, and
Incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research).
As usual, please submit links to both an .Rmd file posted in your GitHub repository and to your code on rpubs.com.  You make work on a small team on this assignment.

In this assignment I will extend on the primary codes provided from chapter 2 in Text Mining with R, by using the sentiment dictionaries bing, nrc, loughran, on Alice in wonderland adventure from the gutenberg package in R.



Using get sentiment function in R I was able to download specific sentiment lexicons with the appropriate measures, where some lexicons requested I agreed to license before downloading. I downloaded sentiments for AFINN from Finn Årup Nielsen, with agreement: http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html.

```{r}
get_sentiments("afinn")
```


Downloaded bing from Bing Liu and collaborators: 
https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
```{r}
get_sentiments("bing")
```

Downloaded nrc from Saif Mohammad and Peter Turney, with agreement:
http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

```{r}
get_sentiments("nrc")
```

The janeaustenr package in R has the six completed books frome Jane Austen. To find the most common joy words in in the book "Emma" by Austen, first the text were unnested to form a tidy format, and the functions grouped by and mutate were used to construct columns for each line and chapter. 

For more information: https://github.com/juliasilge/janeaustenr

```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

head(tidy_books)
```

Secondly, the filtered joy word from "Emma" using the filter function to filter joy words, inner join function to do a sentiment analysis, and the count function to get the count on how many times each word was used.

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

head(nrc_joy)
```

Used bing to find the negative and positive words in the each book by Austen and calculated the net sentiment.

```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

head(jane_austen_sentiment)
```

Data visualization for the net sentiment for each book, the plot was against the index on the x axis which allows us to see how the sentiment changes over trajectory.

```{r}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

Compared the three sentiment dictionaries

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice") #filter words only from the the book "Pride and Prejudice"

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

Visual of the three compared sentiment. The three sentiment dictionaries give different results, FINN gives the highest positive values more variance, Bing has the lowest positive values, and NRC has the least negative value.

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
comparing nrc to bing, bing has higher negative words and nrc has higher positive words.

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```


## Extended Analysis

Looked up a list of books from Lewis Caroll in gutenberg package, the package is a public domain with a collevct of works taht can be used to download and process.

read more: https://github.com/ropensci/gutenbergr
```{r}
gutenberg_works(author== "Carroll, Lewis")
```

Downloaded ALICE'S ADVENTURES IN WONDERLAND by Lewis Carroll from the gutenberg package in R.

```{r}
Alice_in_wonderland_Adv<-gutenberg_download(28885)
```

Tidy formart for the words in the text to be analyzed

```{r}
Alice_in_wonderland_Adv_tidy<- Alice_in_wonderland_Adv %>%
  mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
  unnest_tokens(word, text)
```

Compared the three sentiment NRC, bing, and Afinn in a plot for Alice and wonderland adventure book. The NRC had the least negative words, afinn had the highest positive value, and the bing had the highest amount of negative words.

```{r}
afinn2 <- Alice_in_wonderland_Adv_tidy %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc2 <- bind_rows(
  Alice_in_wonderland_Adv_tidy %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  Alice_in_wonderland_Adv_tidy %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
bind_rows(afinn2, 
          bing_and_nrc2) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

Used loughran as the additional lexicon, used the loughran to look into positive and negative words. For Loughran I had to agree to license before downloading.

```{r}
loughran_posneg <- get_sentiments("loughran") %>% 
  filter(sentiment == "positive" | sentiment =="negative")
```


```{r}
AIWA_loughran <- Alice_in_wonderland_Adv_tidy %>%
  inner_join(loughran_posneg) %>%
  count(index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```


Data visualization for loughran

```{r}
par(mfrow=c(1,2))

ggplot(AIWA_loughran, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) 
```


Loughran sentiment has way more negative words than positive words, which explains why the plot for the Alice in wonderland adv loughran has so many negative sentiment values. The bing plot had more positive values for Alice in wonderland adv compared to the Loughran plot.

```{r}
get_sentiments("loughran") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)
```

## Conclusion

Using sentiment lexicon I was able to analyze words that are most frequently used in documents that are catergorized as positive or negative. In my opinion I would use nrc because the amount of words they have for positive and negative are close and I feel this would help in the avoiding a bias of words. For example Loughran has a way higher count for negative words than positives words, therefore using Loughran most of the time will have more negative sentiment.



## Citation:

for base code:
Silge, J. & Robinson, D. (2016). Welcome to Text Mining with R. O'Reilly Media.

Extented Analysis:

```{r}
citation('gutenbergr')
```